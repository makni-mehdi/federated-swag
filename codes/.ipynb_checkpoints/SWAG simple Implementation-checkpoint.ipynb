{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import itertools\n",
    "from torch.distributions.normal import Normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWAG Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks to Understand the Code:\n",
    "\n",
    "###### Inputs for class SWAG\n",
    "- base, args, kwargs: <br>\n",
    "base is the base model (default=\"VGG16\") we are going to use. args and kwargs contain parameter of the base.\n",
    "- no_cov_mat: <br>\n",
    "When sampling, do we include only the simple SWAG-Diagonal formal. Or add the Low Rank Covariance matrix $DD^T$.\n",
    "- max_num_models: <br>\n",
    "Equivalent to variable $K$ in the paper. Maximum number of columns in deviation matrix.\n",
    "- var_clamp: <br>\n",
    "it is used for calculating variance in torch.clamp. Equivalent to assigning max(computed_var, 1e-30) to variance. Goal: avoid 0 variance errors.\n",
    "\n",
    "###### init for SWAG\n",
    "- n_models: <br>\n",
    "A parameter of the model which should be saved and restored in the state_dict, but not trained by the optimizer, that's why it is registered as buffer.\n",
    "\n",
    "- self.params: <br>\n",
    "The other parameters of the model which will be trained by the optimizer.\n",
    "\n",
    "- n_models: <br>\n",
    "Corresponds to $n$ in the pseudo-code description of the algorithm.\n",
    "\n",
    "###### swag_parameters fct (Not Sure if I understood Correctly !!!)\n",
    "Basically copy the non-trainable parameters of the model to the list params (and then self.params in SWAG class) without taking into account params with key None. Plus create param for the low rank covariance matrix if we want to include it.\n",
    "\n",
    "###### sample fct in SWAG\n",
    "- scale: <br>\n",
    "scaling constant to obtain a valid distribution. (When used it is stored in args) <br>\n",
    "(Example in uncertainty.py: `model.sample(scale=args.scale, cov=sample_with_cov)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quelques questions pour Vincent:\n",
    "- Lien de leur code: <br>\n",
    "https://github.com/wjmaddox/swa_gaussian/blob/master/swag/posteriors/swag.py\n",
    "- Pourquoi dans def sample_fullrank(self, scale, cov, fullrank), ils ont la variable bool fullrank, par contre ils l'utilisent jamais dans la fonction?\n",
    "- Scale, c'est bien la valeur de normalisation ? Elle dépend de quoi, Les entrées de l'utilisateur ? \n",
    "c'est ce qu'il y a dans uncertainty.py: <br>\n",
    "`parser.add_argument(\"--scale\", type=float, default=1.0)`\n",
    "<br>\n",
    "- Je comprends pas trop la diff entre sample_blockwise et sample_fullrank, mmh ont-ils parlé de ça dans le papier?\n",
    "- J'ai cherché un peu dans leur code et à chaque fois qu'ils utilisent la fct sample c'est comme ça: <br>\n",
    "`swag_model.sample(0.0)`\n",
    "<br>\n",
    "ça veut dire quoi ahaha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swag_parameters(module, params, no_cov_mat=True):\n",
    "    for name in list(module._parameters.keys()):\n",
    "        #iterate through parameters of module\n",
    "        if module._parameters[name] is None:\n",
    "            continue\n",
    "        data = module._parameters[name].data\n",
    "        module._parameters.pop(name)\n",
    "        module.register_buffer(\"%s_mean\" % name, data.new(data.size()).zero_())\n",
    "        #Example, non-trainable parameter \\theta_0 called: VGG16s_mean\n",
    "        module.register_buffer(\"%s_sq_mean\" % name, data.new(data.size()).zero_())\n",
    "        # \\theta_{0}^2\n",
    "        \n",
    "        if no_cov_mat is False:\n",
    "            module.register_buffer(\n",
    "                \"%s_cov_mat_sqrt\" % name, data.new_empty((0, data.numel())).zero_()\n",
    "            )\n",
    "            \n",
    "        params.append((module, name))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-45f18af88dee>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-45f18af88dee>\"\u001b[1;36m, line \u001b[1;32m47\u001b[0m\n\u001b[1;33m    mean = mean * self.n_models.item() /\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class SWAG(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, base, no_cov_mat=True, max_num_models=0, var_clamp=1e-30, *args, **kwargs\n",
    "    ):\n",
    "        super(SWAG, self).__init__()\n",
    "        \n",
    "        self.register_buffer(\"n_models\", torch.zeros([1], dtype=torch.long))\n",
    "        self.params = list()\n",
    "        \n",
    "        self.no_cov_mat = no_cov_mat\n",
    "        self.max_num_models = max_num_models\n",
    "        \n",
    "        self.base = base(*args, **kwargs)\n",
    "        self.base.apply(\n",
    "            lambda module: swag_parameters(\n",
    "                module=module, params=self.params, no_cov_mat=self.no_cov_mat\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, base, *args, **kwargs):\n",
    "        return self.base(*args, **kwargs)\n",
    "    \n",
    "    \n",
    "    def sample(self, scale=1.0, cov=False, seed=None, block=False, fullrank=True):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            #If we need to rerun an experiment we should have a fixed seed. Otherwise torch chooses.\n",
    "        if not block:\n",
    "            self.sample_fullrank(scale, cov, fullrank)\n",
    "        else:\n",
    "            self.sample_blockwise(scale, cov, fullrank)\n",
    "        \n",
    "        \n",
    "    def sample_fullrank(scale, cov, fullrank):\n",
    "        pass\n",
    "    def sample_blockwise(scale, cov, fullrank):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def collect_model(self, base_model):\n",
    "        for (module, name), base_param in zip(self.params, base_model.parameters()):\n",
    "            mean = module.__getattr__(\"%s_mean\" % name) #\\theta_0\n",
    "            sq_mean = module.__getattr__(\"%s_sq_mean\" % name) #\\theta_0^2\n",
    "            \n",
    "            # First Moment\n",
    "            mean = mean * self.n_models.item() / \n",
    "            \n",
    "            \n",
    "    def generate_mean_var_covar(self):\n",
    "        mean_list = []\n",
    "        var_list = []\n",
    "        cov_mat_root_list = []\n",
    "        for module, name in self.params:\n",
    "            mean = module.__getattr__(\"%s_mean\" % name)\n",
    "            sq_mean = module.__getattr__(\"%s_sq_mean\" % name)\n",
    "            cov_mat_sqrt = module.__getattr__(\"%s_cov_mat_sqrt\" % name)\n",
    "            \n",
    "            mean_list.append(mean)\n",
    "            var_list.append(sq_mean - mean ** 2.0)\n",
    "            cov_mat_root_list.append(cov_mat_sqrt)\n",
    "        return mean_list, var_list, cov_mat_root_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't forget:\n",
    "To sample from SWAG we use the following identity:\n",
    "\\begin{equation}\n",
    "\\tilde{\\theta} = \\theta_{\\text{SWA}} + \\frac{1}{\\sqrt{2}} \\cdot \\Sigma_{\\text{diag}}^{\\frac{1}{2}}\n",
    "z_1 + \\frac{1}{\\sqrt{2(K-1)}}\\hat{D} z_2, \\quad \\text{where} \\quad z_1 \\sim \\mathcal{N}(0, I_d), z_2 \\sim \\mathcal{N}(0, I_K)\n",
    "\\end{equation} "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
