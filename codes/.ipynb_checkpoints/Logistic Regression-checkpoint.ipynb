{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = sklearn.datasets.load_breast_cancer()\n",
    "bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numpy, y_numpy = bc.data, bc.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y_numpy, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)  # J'ai juste modifié en enlevant l'option fit, car il faut transformer les données avec les paramètres appris sur le train (ex: si on a une seule donnée test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size, output_size = n_features, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.lin = nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.lin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "n_iters = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 500, loss = 0.0527, accuracy = 0.987\n",
      "epoch 1000, loss = 0.0477, accuracy = 0.987\n",
      "epoch 1500, loss = 0.0451, accuracy = 0.991\n",
      "epoch 2000, loss = 0.0432, accuracy = 0.991\n",
      "epoch 2500, loss = 0.0418, accuracy = 0.991\n",
      "epoch 3000, loss = 0.0406, accuracy = 0.989\n",
      "epoch 3500, loss = 0.0396, accuracy = 0.989\n",
      "epoch 4000, loss = 0.0387, accuracy = 0.989\n",
      "epoch 4500, loss = 0.0380, accuracy = 0.989\n",
      "epoch 5000, loss = 0.0372, accuracy = 0.989\n",
      "epoch 5500, loss = 0.0366, accuracy = 0.989\n",
      "epoch 6000, loss = 0.0360, accuracy = 0.989\n",
      "epoch 6500, loss = 0.0354, accuracy = 0.989\n",
      "epoch 7000, loss = 0.0348, accuracy = 0.989\n",
      "epoch 7500, loss = 0.0343, accuracy = 0.989\n",
      "epoch 8000, loss = 0.0338, accuracy = 0.989\n",
      "epoch 8500, loss = 0.0334, accuracy = 0.989\n",
      "epoch 9000, loss = 0.0329, accuracy = 0.989\n",
      "epoch 9500, loss = 0.0325, accuracy = 0.989\n",
      "epoch 10000, loss = 0.0321, accuracy = 0.989\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        accuracy = ((((y_pred > 0.5) * 1.0) == y_train) * 1).sum() / float(y_train.shape[0])\n",
    "        print(f'epoch {epoch + 1}, loss = {loss.item():.4f}, accuracy = {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Accuracy and loss using standard SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.09062103927135468\n",
      "accuracy = 97.36841583251953\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat_test = model(X_test)\n",
    "    loss = criterion(y_hat_test, y_test)\n",
    "    print(f'loss = {loss.item()}')\n",
    "    y_hat_cls = (y_hat_test > 0.5)\n",
    "    accuracy = (y_hat_cls == y_test).sum() / float(y_test.shape[0]) * 100\n",
    "    print(f'accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWA Implementation\n",
    "- Now that the model is trained and we have a decent starting point for the weights. We run Stochastic Weight Averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save old_model so that we do not write the obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = LogisticRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_old, param_model in zip(old_model.parameters(), model.parameters()):\n",
    "    param_old.data = param_model.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New swa_model with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "swa_model = LogisticRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_swa, param_model in zip(swa_model.parameters(), model.parameters()):\n",
    "    param_swa.data = param_model.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepcopy the weights so they do not change when we udpate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "swa_iters = 10000\n",
    "swa_lr = 0.1\n",
    "c_update = 10 # update weights every c_update epochs.\n",
    "swa_n = 100 # used for moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following the definition of schedule in the swa implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = []\n",
    "\n",
    "def schedule(epoch):\n",
    "    t = epoch / swa_iters\n",
    "    lr_ratio = swa_lr / learning_rate\n",
    "    if t <= 0.5:\n",
    "        factor = 1.0\n",
    "    elif t <= 0.9:\n",
    "        factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
    "    else:\n",
    "        factor = lr_ratio\n",
    "    lr = learning_rate * factor \n",
    "    # print('lr = ', lr)\n",
    "    learning_rate_list.append(lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameters of net1 with input from net2\n",
    "def moving_average(net1, net2, alpha=1):\n",
    "    for param1, param2 in zip(net1.parameters(), net2.parameters()):\n",
    "        param1.data = param1.data * (1 - alpha) + param2.data * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_iters, n_iters + swa_iters):\n",
    "    # swa lr schedule config\n",
    "    lr = schedule(epoch - n_iters)\n",
    "    adjust_learning_rate(optimizer, lr)\n",
    "    \n",
    "    # training model\n",
    "    optimizer.zero_grad()  # même si ça ne change rien, j'aime bien mettre à 0 le gradient juste avant de commencer les calculs :-)\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # updating swa_weight with moving_average\n",
    "    if (epoch - n_iters + 1) % c_update == 0:\n",
    "        moving_average(swa_model, model, 1 / (swa_n + 1))\n",
    "        swa_n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1481ed5de20>"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(learning_rate_list, label='Learning rate')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.09062103927135468\n",
      "accuracy = 97.36841583251953\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat_test = old_model(X_test)\n",
    "    loss = criterion(y_hat_test, y_test)\n",
    "    print(f'loss = {loss.item()}')\n",
    "    y_hat_cls = (y_hat_test > 0.5)\n",
    "    accuracy = (y_hat_cls == y_test).sum() / float(y_test.shape[0]) * 100\n",
    "    print(f'accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swa_model results thanks to averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.10540612787008286\n",
      "accuracy = 96.49122619628906\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat_test = swa_model(X_test)\n",
    "    loss = criterion(y_hat_test, y_test)\n",
    "    print(f'loss = {loss.item()}')\n",
    "    y_hat_cls = (y_hat_test > 0.5) * 1\n",
    "    accuracy = (y_hat_cls == y_test).sum() / float(y_test.shape[0]) * 100\n",
    "    print(f'accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current model obtained with running more SGD updates on old_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.11873631179332733\n",
      "accuracy = 96.49122619628906\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat_test = model(X_test)\n",
    "    loss = criterion(y_hat_test, y_test)\n",
    "    print(f'loss = {loss.item()}')\n",
    "    y_hat_cls = (y_hat_test > 0.5) * 1\n",
    "    accuracy = (y_hat_cls == y_test).sum() / float(y_test.shape[0]) * 100\n",
    "    print(f'accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rk about SWA: \n",
    "- Usually the loss of SWA is between the latest and oldest models but with these parameters of learning_rate, it is very slightly better. \n",
    "Maybe it can perform even better when it is is not a strongly convex problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWAG implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the old_model parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "swag_model = LogisticRegression(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "for swag_param, param in zip(swag_model.parameters(), old_model.parameters()):\n",
    "    swag_param.data = param.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(swag_model.parameters(), lr=learning_rate*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the thetas as described in the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.cat([param.data.view(-1) for param in swag_model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq2theta = torch.cat([param.data.view(-1)**2 for param in swag_model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.5386e-02,  1.1453e-01,  5.5398e-01, -6.7993e-04,  3.7435e-02,\n",
       "         3.7240e+00, -2.0701e+00, -4.1622e+00,  1.2787e+00, -1.4751e+00,\n",
       "        -4.8662e+00,  7.1799e-01, -1.4117e-01, -3.1402e+00, -1.0379e+00,\n",
       "         2.2421e-01,  2.4057e+00, -1.5659e+00,  1.6313e+00,  1.9631e+00,\n",
       "        -2.0449e+00, -3.2375e+00, -5.1910e-03, -2.3215e+00,  2.3225e-01,\n",
       "         1.8919e-01, -3.3567e+00, -1.1377e+00, -3.5161e+00,  6.5904e-02,\n",
       "        -7.3247e-01])"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta # example of theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2754e-03, 1.3116e-02, 3.0690e-01, 4.6230e-07, 1.4014e-03, 1.3868e+01,\n",
       "        4.2854e+00, 1.7324e+01, 1.6350e+00, 2.1760e+00, 2.3680e+01, 5.1551e-01,\n",
       "        1.9929e-02, 9.8612e+00, 1.0772e+00, 5.0270e-02, 5.7875e+00, 2.4519e+00,\n",
       "        2.6612e+00, 3.8539e+00, 4.1818e+00, 1.0482e+01, 2.6947e-05, 5.3894e+00,\n",
       "        5.3942e-02, 3.5792e-02, 1.1268e+01, 1.2944e+00, 1.2363e+01, 4.3433e-03,\n",
       "        5.3651e-01])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq2theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input of Swag as described in pseudocode\n",
    "T = 2000\n",
    "c = 50\n",
    "K = 20\n",
    "D = torch.empty((theta.shape[0], K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the SWAG loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(T):\n",
    "    \n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    theta_i = torch.cat([param.data.view(-1) for param in swag_model.parameters()])\n",
    "    if i % c == 0:\n",
    "        n = i / c\n",
    "\n",
    "        theta = (n * theta + theta_i) / (n + 1)\n",
    "        sq2theta = (n * sq2theta + theta_i**2) / (n + 1)\n",
    "                    \n",
    "        D = torch.cat((D[:, 1:], (theta_i - theta).view(-1, 1)), dim=1)\n",
    "        \n",
    "theta_swa = theta\n",
    "sigma_diag = torch.clamp(sq2theta - theta**2, 1e-16)\n",
    "D_hat = D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.5386e-02,  1.1453e-01,  5.5398e-01, -6.7993e-04,  3.7435e-02,\n",
       "         3.7240e+00, -2.0701e+00, -4.1622e+00,  1.2787e+00, -1.4751e+00,\n",
       "        -4.8662e+00,  7.1799e-01, -1.4117e-01, -3.1402e+00, -1.0379e+00,\n",
       "         2.2421e-01,  2.4057e+00, -1.5659e+00,  1.6313e+00,  1.9631e+00,\n",
       "        -2.0449e+00, -3.2375e+00, -5.1910e-03, -2.3215e+00,  2.3225e-01,\n",
       "         1.8919e-01, -3.3567e+00, -1.1377e+00, -3.5161e+00,  6.5904e-02,\n",
       "        -7.3247e-01])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_swa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1223e-09, 1.0245e-08, 3.2783e-07, 2.8422e-13, 4.6566e-10, 1.0000e-16,\n",
       "        4.7684e-06, 1.0000e-16, 3.5763e-07, 4.7684e-07, 1.0000e-16, 1.0000e-16,\n",
       "        1.0000e-16, 1.0000e-16, 1.0000e-16, 7.0781e-08, 4.2915e-06, 1.0000e-16,\n",
       "        1.0000e-16, 5.2452e-06, 1.0000e-16, 7.6294e-06, 1.4552e-11, 1.0000e-16,\n",
       "        2.6077e-08, 1.0000e-16, 1.9073e-06, 1.0000e-16, 1.0000e-16, 9.3132e-10,\n",
       "        4.7684e-07])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 20])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          7.4506e-09,  7.4506e-09,  7.4506e-09,  1.4901e-08,  1.4901e-08,\n",
       "          2.2352e-08,  2.2352e-08,  2.2352e-08,  2.9802e-08,  3.7253e-08],\n",
       "        [-1.4901e-08, -7.4506e-09, -7.4506e-09,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  7.4506e-09,  7.4506e-09,  7.4506e-09,  1.4901e-08,\n",
       "          1.4901e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,\n",
       "          1.4901e-08,  1.4901e-08,  1.4901e-08,  1.4901e-08,  2.2352e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          5.9605e-08,  5.9605e-08,  5.9605e-08,  1.1921e-07,  1.1921e-07,\n",
       "          1.1921e-07,  1.1921e-07,  1.7881e-07,  1.7881e-07,  1.7881e-07],\n",
       "        [ 5.8208e-11,  5.8208e-11,  5.8208e-11,  5.8208e-11,  0.0000e+00,\n",
       "          0.0000e+00, -5.8208e-11, -5.8208e-11, -1.1642e-10, -1.1642e-10,\n",
       "         -1.7462e-10, -1.7462e-10, -1.7462e-10, -2.3283e-10, -2.3283e-10,\n",
       "         -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10, -2.3283e-10],\n",
       "        [ 7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,\n",
       "          7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,\n",
       "          7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,\n",
       "          7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09,  7.4506e-09],\n",
       "        [-2.3842e-07, -2.3842e-07, -4.7684e-07, -4.7684e-07, -7.1526e-07,\n",
       "         -7.1526e-07, -9.5367e-07, -9.5367e-07, -9.5367e-07, -9.5367e-07,\n",
       "         -1.1921e-06, -1.1921e-06, -1.1921e-06, -1.1921e-06, -1.1921e-06,\n",
       "         -9.5367e-07, -7.1526e-07, -7.1526e-07, -4.7684e-07, -4.7684e-07],\n",
       "        [-2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
       "         -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
       "         -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -4.7684e-07,\n",
       "         -7.1526e-07, -9.5367e-07, -9.5367e-07, -9.5367e-07, -9.5367e-07],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -4.7684e-07,  0.0000e+00,  0.0000e+00,  4.7684e-07,  4.7684e-07,\n",
       "          9.5367e-07,  9.5367e-07,  1.4305e-06,  1.4305e-06,  1.4305e-06],\n",
       "        [-2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
       "         -2.3842e-07, -1.1921e-07, -1.1921e-07,  0.0000e+00,  0.0000e+00,\n",
       "          1.1921e-07,  1.1921e-07,  1.1921e-07,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07],\n",
       "        [-2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
       "         -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
       "         -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
       "         -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07],\n",
       "        [-4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07,\n",
       "         -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07,\n",
       "         -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07,\n",
       "         -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07],\n",
       "        [-1.1921e-07, -1.1921e-07, -5.9605e-08, -5.9605e-08, -5.9605e-08,\n",
       "         -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07,\n",
       "         -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07,\n",
       "         -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-2.3842e-07, -2.3842e-07,  0.0000e+00,  0.0000e+00,  2.3842e-07,\n",
       "          2.3842e-07,  4.7684e-07,  4.7684e-07,  4.7684e-07,  4.7684e-07,\n",
       "          7.1526e-07,  7.1526e-07,  7.1526e-07,  7.1526e-07,  9.5367e-07,\n",
       "          9.5367e-07,  9.5367e-07,  9.5367e-07,  9.5367e-07,  9.5367e-07],\n",
       "        [ 2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07],\n",
       "        [ 2.9802e-08,  2.9802e-08,  4.4703e-08,  4.4703e-08,  5.9605e-08,\n",
       "          5.9605e-08,  7.4506e-08,  7.4506e-08,  7.4506e-08,  7.4506e-08,\n",
       "          8.9407e-08,  8.9407e-08,  8.9407e-08,  8.9407e-08,  1.0431e-07,\n",
       "          1.0431e-07,  1.0431e-07,  1.0431e-07,  1.0431e-07,  1.1921e-07],\n",
       "        [-4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07,\n",
       "         -4.7684e-07, -4.7684e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3842e-07,  4.7684e-07,\n",
       "          4.7684e-07,  4.7684e-07,  4.7684e-07,  4.7684e-07,  4.7684e-07],\n",
       "        [ 1.1921e-07,  1.1921e-07,  2.3842e-07,  3.5763e-07,  3.5763e-07,\n",
       "          4.7684e-07,  4.7684e-07,  5.9605e-07,  5.9605e-07,  5.9605e-07,\n",
       "          7.1526e-07,  7.1526e-07,  7.1526e-07,  8.3447e-07,  8.3447e-07,\n",
       "          8.3447e-07,  8.3447e-07,  9.5367e-07,  9.5367e-07,  9.5367e-07],\n",
       "        [ 1.1921e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  2.3842e-07,  1.1921e-07],\n",
       "        [ 4.7684e-07,  4.7684e-07,  5.9605e-07,  5.9605e-07,  7.1526e-07,\n",
       "          7.1526e-07,  8.3447e-07,  8.3447e-07,  8.3447e-07,  8.3447e-07,\n",
       "          9.5367e-07,  9.5367e-07,  9.5367e-07,  9.5367e-07,  1.0729e-06,\n",
       "          1.0729e-06,  1.0729e-06,  1.0729e-06,  1.0729e-06,  1.1921e-06],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3842e-07,  2.3842e-07,\n",
       "          4.7684e-07,  4.7684e-07,  4.7684e-07,  7.1526e-07,  7.1526e-07],\n",
       "        [-7.1526e-07, -9.5367e-07, -9.5367e-07, -9.5367e-07, -9.5367e-07,\n",
       "         -9.5367e-07, -9.5367e-07, -1.1921e-06, -1.1921e-06, -1.1921e-06,\n",
       "         -1.1921e-06, -1.1921e-06, -1.1921e-06, -1.1921e-06, -1.1921e-06,\n",
       "         -1.1921e-06, -1.1921e-06, -1.1921e-06, -1.1921e-06, -1.1921e-06],\n",
       "        [-9.3132e-10, -9.3132e-10, -9.3132e-10, -9.3132e-10, -4.6566e-10,\n",
       "         -9.3132e-10, -9.3132e-10, -9.3132e-10, -9.3132e-10, -9.3132e-10,\n",
       "         -9.3132e-10, -9.3132e-10, -9.3132e-10, -9.3132e-10, -9.3132e-10,\n",
       "         -9.3132e-10, -9.3132e-10, -9.3132e-10, -9.3132e-10, -9.3132e-10],\n",
       "        [-4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07,\n",
       "         -4.7684e-07, -4.7684e-07, -4.7684e-07, -2.3842e-07, -2.3842e-07,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  4.7684e-07,  7.1526e-07],\n",
       "        [ 2.9802e-08,  2.9802e-08,  4.4703e-08,  4.4703e-08,  5.9605e-08,\n",
       "          5.9605e-08,  7.4506e-08,  7.4506e-08,  7.4506e-08,  7.4506e-08,\n",
       "          8.9407e-08,  8.9407e-08,  8.9407e-08,  8.9407e-08,  8.9407e-08,\n",
       "          7.4506e-08,  5.9605e-08,  5.9605e-08,  4.4703e-08,  4.4703e-08],\n",
       "        [ 4.4703e-08,  4.4703e-08,  4.4703e-08,  2.9802e-08,  2.9802e-08,\n",
       "          1.4901e-08,  1.4901e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -1.4901e-08, -1.4901e-08, -1.4901e-08, -2.9802e-08, -4.4703e-08,\n",
       "         -4.4703e-08, -4.4703e-08, -4.4703e-08, -4.4703e-08, -4.4703e-08],\n",
       "        [ 2.3842e-07,  2.3842e-07,  0.0000e+00,  0.0000e+00, -2.3842e-07,\n",
       "         -2.3842e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07, -4.7684e-07,\n",
       "         -7.1526e-07, -7.1526e-07, -7.1526e-07, -7.1526e-07, -9.5367e-07,\n",
       "         -9.5367e-07, -9.5367e-07, -9.5367e-07, -9.5367e-07, -7.1526e-07],\n",
       "        [-1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07, -1.1921e-07,\n",
       "         -1.1921e-07, -1.1921e-07, -1.1921e-07,  0.0000e+00,  0.0000e+00,\n",
       "          1.1921e-07,  1.1921e-07,  1.1921e-07,  2.3842e-07,  2.3842e-07,\n",
       "          2.3842e-07,  2.3842e-07,  2.3842e-07,  3.5763e-07,  4.7684e-07],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  2.3842e-07,  4.7684e-07,  4.7684e-07,  7.1526e-07],\n",
       "        [-7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09,\n",
       "         -7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09, -7.4506e-09,\n",
       "         -7.4506e-09, -7.4506e-09, -7.4506e-09,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  7.4506e-09,  7.4506e-09,  7.4506e-09],\n",
       "        [ 0.0000e+00,  0.0000e+00, -5.9605e-08, -1.1921e-07, -1.1921e-07,\n",
       "         -1.7881e-07, -1.7881e-07, -2.3842e-07, -2.3842e-07, -2.3842e-07,\n",
       "         -2.9802e-07, -2.9802e-07, -2.9802e-07, -3.5763e-07, -3.5763e-07,\n",
       "         -3.5763e-07, -3.5763e-07, -4.1723e-07, -4.1723e-07, -4.1723e-07]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing SWAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 10\n",
    "d = sigma_diag.shape[0]\n",
    "mean_d = np.zeros(d)\n",
    "cov_d = np.eye(d)\n",
    "mean_K = np.zeros(K)\n",
    "cov_K = np.eye(K)\n",
    "# for i in range(S):\n",
    "#     theta_hat_i = theta_swa + 1 / np.sqrt(2) * (sigma_diag ** 0.5) * np.random.normal(mean_d, cov_d) + \\\n",
    "#         1 / np.sqrt(2 * (K - 1)) * np.random.normal(mean_K, cov_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3854e-15,  3.0531e-15,  2.6201e-14, -4.2067e-17,  1.3878e-15,\n",
       "         -1.4744e-13, -1.3856e-13,  1.8119e-13,  4.1744e-14, -4.4409e-14,\n",
       "         -8.8818e-14, -2.2204e-14,  0.0000e+00,  1.6875e-13,  4.4409e-14,\n",
       "          1.9429e-14,  7.4607e-14,  1.6342e-13,  3.9968e-14,  1.9984e-13,\n",
       "          8.7041e-14, -2.2204e-13, -1.7347e-16,  6.3949e-14,  1.1990e-14,\n",
       "         -7.4385e-15, -1.5987e-13,  5.4179e-14,  5.6843e-14,  4.9960e-16,\n",
       "         -7.0610e-14],\n",
       "        [ 3.0531e-15,  3.2196e-15,  1.9096e-14, -3.9031e-17,  1.2212e-15,\n",
       "         -1.6875e-13, -9.7700e-14,  1.1013e-13,  3.7303e-14, -3.9080e-14,\n",
       "         -7.8160e-14, -1.9984e-14,  0.0000e+00,  1.5810e-13,  3.9080e-14,\n",
       "          1.7542e-14,  5.3291e-14,  1.4833e-13,  3.8192e-14,  1.7941e-13,\n",
       "          5.5067e-14, -2.0428e-13, -1.5266e-16,  4.2633e-14,  1.2990e-14,\n",
       "         -6.6613e-15, -1.5277e-13,  4.0856e-14,  3.3751e-14,  0.0000e+00,\n",
       "         -6.4393e-14],\n",
       "        [ 2.6201e-14,  1.9096e-14,  1.6342e-13, -2.6715e-16,  8.8818e-15,\n",
       "         -9.9476e-13, -8.3844e-13,  1.0800e-12,  2.6290e-13, -2.8422e-13,\n",
       "         -5.6843e-13, -1.4211e-13,  0.0000e+00,  1.0658e-12,  2.8422e-13,\n",
       "          1.2257e-13,  4.5475e-13,  1.0374e-12,  2.6290e-13,  1.2648e-12,\n",
       "          5.1159e-13, -1.4211e-12, -1.1102e-15,  3.6948e-13,  7.9936e-14,\n",
       "         -4.6185e-14, -1.0232e-12,  3.2685e-13,  3.2685e-13,  2.6645e-15,\n",
       "         -4.4764e-13],\n",
       "        [-4.2067e-17, -3.9031e-17, -2.6715e-16,  5.1838e-19, -1.6914e-17,\n",
       "          2.2066e-15,  1.3739e-15, -1.5821e-15, -4.9266e-16,  5.4123e-16,\n",
       "          1.0825e-15,  2.7756e-16,  0.0000e+00, -2.0678e-15, -5.4123e-16,\n",
       "         -2.3419e-16, -7.3552e-16, -1.9706e-15, -5.2042e-16, -2.4147e-15,\n",
       "         -7.7716e-16,  2.7617e-15,  2.1142e-18, -5.5511e-16, -1.7174e-16,\n",
       "          8.5869e-17,  2.0123e-15, -5.4817e-16, -4.4409e-16, -4.3368e-19,\n",
       "          8.5001e-16],\n",
       "        [ 1.3878e-15,  1.2212e-15,  8.8818e-15, -1.6914e-17,  1.1102e-15,\n",
       "         -1.1902e-13, -6.2172e-14,  4.9738e-14,  2.6645e-15, -3.5527e-14,\n",
       "         -7.1054e-14, -1.6431e-14,  0.0000e+00,  7.8160e-14,  3.5527e-14,\n",
       "          1.1657e-14, -7.1054e-15,  9.1482e-14,  3.3751e-14,  1.2879e-13,\n",
       "          2.4869e-14, -1.6342e-13, -1.3531e-16, -1.4211e-14,  9.6589e-15,\n",
       "         -8.8818e-16, -7.6383e-14,  1.0658e-14,  1.4211e-14, -5.5511e-16,\n",
       "         -3.6859e-14],\n",
       "        [-1.4744e-13, -1.6875e-13, -9.9476e-13,  2.2066e-15, -1.1902e-13,\n",
       "          1.4722e-11,  6.2528e-12, -4.5475e-12, -8.8107e-13,  3.8085e-12,\n",
       "          7.6170e-12,  1.8048e-12,  0.0000e+00, -9.7771e-12, -3.8085e-12,\n",
       "         -1.3323e-12,  5.6843e-14, -1.0630e-11, -3.7232e-12, -1.4467e-11,\n",
       "         -2.3874e-12,  1.8076e-11,  1.4544e-14,  1.0232e-12, -1.1582e-12,\n",
       "          1.8829e-13,  9.6634e-12, -1.3927e-12, -1.0800e-12,  6.3949e-14,\n",
       "          4.3627e-12],\n",
       "        [-1.3856e-13, -9.7700e-14, -8.3844e-13,  1.3739e-15, -6.2172e-14,\n",
       "          6.2528e-12,  5.1728e-12, -5.9117e-12, -9.3792e-13,  1.9895e-12,\n",
       "          3.9790e-12,  9.5213e-13,  0.0000e+00, -5.9117e-12, -1.9895e-12,\n",
       "         -7.5673e-13, -1.4779e-12, -6.1675e-12, -1.8474e-12, -8.0433e-12,\n",
       "         -2.7853e-12,  9.4929e-12,  7.6605e-15, -9.0949e-13, -5.1514e-13,\n",
       "          1.8829e-13,  5.6843e-12, -1.4495e-12, -1.8190e-12,  1.7764e-15,\n",
       "          2.5864e-12],\n",
       "        [ 1.8119e-13,  1.1013e-13,  1.0800e-12, -1.5821e-15,  4.9738e-14,\n",
       "         -4.5475e-12, -5.9117e-12,  8.6402e-12,  1.6485e-12, -1.5916e-12,\n",
       "         -3.1832e-12, -7.9581e-13,  0.0000e+00,  6.3665e-12,  1.5916e-12,\n",
       "          7.1765e-13,  3.2969e-12,  6.1391e-12,  1.4211e-12,  7.3328e-12,\n",
       "          3.8654e-12, -7.9581e-12, -6.2172e-15,  2.7285e-12,  3.8369e-13,\n",
       "         -3.0553e-13, -6.0254e-12,  2.1600e-12,  2.6148e-12,  3.5527e-14,\n",
       "         -2.6716e-12],\n",
       "        [ 4.1744e-14,  3.7303e-14,  2.6290e-13, -4.9266e-16,  2.6645e-15,\n",
       "         -8.8107e-13, -9.3792e-13,  1.6485e-12,  8.1002e-13, -8.5265e-14,\n",
       "         -1.7053e-13, -8.5265e-14,  0.0000e+00,  1.6769e-12,  8.5265e-14,\n",
       "          1.2434e-13,  1.5064e-12,  1.2079e-12,  8.5265e-14,  1.0800e-12,\n",
       "          7.9581e-13, -8.5265e-13, -4.4409e-16,  1.3642e-12,  6.0396e-14,\n",
       "         -1.2790e-13, -1.6200e-12,  7.2475e-13,  4.5475e-13,  1.5099e-14,\n",
       "         -5.8265e-13],\n",
       "        [-4.4409e-14, -3.9080e-14, -2.8422e-13,  5.4123e-16, -3.5527e-14,\n",
       "          3.8085e-12,  1.9895e-12, -1.5916e-12, -8.5265e-14,  1.1369e-12,\n",
       "          2.2737e-12,  5.2580e-13,  0.0000e+00, -2.5011e-12, -1.1369e-12,\n",
       "         -3.7303e-13,  2.2737e-13, -2.9274e-12, -1.0800e-12, -4.1211e-12,\n",
       "         -7.9581e-13,  5.2296e-12,  4.3299e-15,  4.5475e-13, -3.0909e-13,\n",
       "          2.8422e-14,  2.4443e-12, -3.4106e-13, -4.5475e-13,  1.7764e-14,\n",
       "          1.1795e-12],\n",
       "        [-8.8818e-14, -7.8160e-14, -5.6843e-13,  1.0825e-15, -7.1054e-14,\n",
       "          7.6170e-12,  3.9790e-12, -3.1832e-12, -1.7053e-13,  2.2737e-12,\n",
       "          4.5475e-12,  1.0516e-12,  0.0000e+00, -5.0022e-12, -2.2737e-12,\n",
       "         -7.4607e-13,  4.5475e-13, -5.8549e-12, -2.1600e-12, -8.2423e-12,\n",
       "         -1.5916e-12,  1.0459e-11,  8.6597e-15,  9.0949e-13, -6.1817e-13,\n",
       "          5.6843e-14,  4.8885e-12, -6.8212e-13, -9.0949e-13,  3.5527e-14,\n",
       "          2.3590e-12],\n",
       "        [-2.2204e-14, -1.9984e-14, -1.4211e-13,  2.7756e-16, -1.6431e-14,\n",
       "          1.8048e-12,  9.5213e-13, -7.9581e-13, -8.5265e-14,  5.2580e-13,\n",
       "          1.0516e-12,  2.5224e-13,  0.0000e+00, -1.2363e-12, -5.2580e-13,\n",
       "         -1.7764e-13,  2.8422e-14, -1.4069e-12, -4.9738e-13, -1.9469e-12,\n",
       "         -3.9790e-13,  2.4443e-12,  2.0262e-15,  1.4211e-13, -1.4566e-13,\n",
       "          2.0428e-14,  1.2079e-12, -1.9185e-13, -2.2737e-13,  7.5495e-15,\n",
       "          5.7199e-13],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00],\n",
       "        [ 1.6875e-13,  1.5810e-13,  1.0658e-12, -2.0678e-15,  7.8160e-14,\n",
       "         -9.7771e-12, -5.9117e-12,  6.3665e-12,  1.6769e-12, -2.5011e-12,\n",
       "         -5.0022e-12, -1.2363e-12,  0.0000e+00,  8.6402e-12,  2.5011e-12,\n",
       "          1.0232e-12,  2.3306e-12,  8.4697e-12,  2.4158e-12,  1.0687e-11,\n",
       "          3.1264e-12, -1.2449e-11, -9.6589e-15,  1.5348e-12,  7.6739e-13,\n",
       "         -3.1264e-13, -8.4128e-12,  2.0179e-12,  1.8190e-12, -8.8818e-15,\n",
       "         -3.6096e-12],\n",
       "        [ 4.4409e-14,  3.9080e-14,  2.8422e-13, -5.4123e-16,  3.5527e-14,\n",
       "         -3.8085e-12, -1.9895e-12,  1.5916e-12,  8.5265e-14, -1.1369e-12,\n",
       "         -2.2737e-12, -5.2580e-13,  0.0000e+00,  2.5011e-12,  1.1369e-12,\n",
       "          3.7303e-13, -2.2737e-13,  2.9274e-12,  1.0800e-12,  4.1211e-12,\n",
       "          7.9581e-13, -5.2296e-12, -4.3299e-15, -4.5475e-13,  3.0909e-13,\n",
       "         -2.8422e-14, -2.4443e-12,  3.4106e-13,  4.5475e-13, -1.7764e-14,\n",
       "         -1.1795e-12],\n",
       "        [ 1.9429e-14,  1.7542e-14,  1.2257e-13, -2.3419e-16,  1.1657e-14,\n",
       "         -1.3323e-12, -7.5673e-13,  7.1765e-13,  1.2434e-13, -3.7303e-13,\n",
       "         -7.4607e-13, -1.7764e-13,  0.0000e+00,  1.0232e-12,  3.7303e-13,\n",
       "          1.3567e-13,  1.1013e-13,  1.0925e-12,  3.5527e-13,  1.4584e-12,\n",
       "          3.5527e-13, -1.7764e-12, -1.4294e-15,  2.1316e-14,  1.0658e-13,\n",
       "         -2.5535e-14, -9.9476e-13,  1.9718e-13,  2.0961e-13, -3.7748e-15,\n",
       "         -4.5297e-13],\n",
       "        [ 7.4607e-14,  5.3291e-14,  4.5475e-13, -7.3552e-16, -7.1054e-15,\n",
       "          5.6843e-14, -1.4779e-12,  3.2969e-12,  1.5064e-12,  2.2737e-13,\n",
       "          4.5475e-13,  2.8422e-14,  0.0000e+00,  2.3306e-12, -2.2737e-13,\n",
       "          1.1013e-13,  3.1832e-12,  1.3074e-12, -2.2737e-13,  6.5370e-13,\n",
       "          1.5348e-12,  2.2737e-13,  6.6613e-16,  2.8990e-12, -1.7764e-14,\n",
       "         -2.4158e-13, -2.2169e-12,  1.3358e-12,  9.0949e-13,  4.0856e-14,\n",
       "         -7.1054e-13],\n",
       "        [ 1.6342e-13,  1.4833e-13,  1.0374e-12, -1.9706e-15,  9.1482e-14,\n",
       "         -1.0630e-11, -6.1675e-12,  6.1391e-12,  1.2079e-12, -2.9274e-12,\n",
       "         -5.8549e-12, -1.4069e-12,  0.0000e+00,  8.4697e-12,  2.9274e-12,\n",
       "          1.0925e-12,  1.3074e-12,  8.9102e-12,  2.7995e-12,  1.1667e-11,\n",
       "          3.0127e-12, -1.4097e-11, -1.1269e-14,  5.6843e-13,  8.4732e-13,\n",
       "         -2.3981e-13, -8.2423e-12,  1.7479e-12,  1.7906e-12, -2.3981e-14,\n",
       "         -3.7232e-12],\n",
       "        [ 3.9968e-14,  3.8192e-14,  2.6290e-13, -5.2042e-16,  3.3751e-14,\n",
       "         -3.7232e-12, -1.8474e-12,  1.4211e-12,  8.5265e-14, -1.0800e-12,\n",
       "         -2.1600e-12, -4.9738e-13,  0.0000e+00,  2.4158e-12,  1.0800e-12,\n",
       "          3.5527e-13, -2.2737e-13,  2.7995e-12,  1.0516e-12,  3.9222e-12,\n",
       "          7.1054e-13, -5.0022e-12, -4.1078e-15, -4.8317e-13,  3.0020e-13,\n",
       "         -2.8422e-14, -2.3874e-12,  2.9843e-13,  3.6948e-13, -1.7764e-14,\n",
       "         -1.1298e-12],\n",
       "        [ 1.9984e-13,  1.7941e-13,  1.2648e-12, -2.4147e-15,  1.2879e-13,\n",
       "         -1.4467e-11, -8.0433e-12,  7.3328e-12,  1.0800e-12, -4.1211e-12,\n",
       "         -8.2423e-12, -1.9469e-12,  0.0000e+00,  1.0687e-11,  4.1211e-12,\n",
       "          1.4584e-12,  6.5370e-13,  1.1667e-11,  3.9222e-12,  1.5788e-11,\n",
       "          3.6380e-12, -1.9440e-11, -1.5765e-14, -2.8422e-13,  1.1617e-12,\n",
       "         -2.3270e-13, -1.0402e-11,  1.9185e-12,  2.1316e-12, -4.7962e-14,\n",
       "         -4.8033e-12],\n",
       "        [ 8.7041e-14,  5.5067e-14,  5.1159e-13, -7.7716e-16,  2.4869e-14,\n",
       "         -2.3874e-12, -2.7853e-12,  3.8654e-12,  7.9581e-13, -7.9581e-13,\n",
       "         -1.5916e-12, -3.9790e-13,  0.0000e+00,  3.1264e-12,  7.9581e-13,\n",
       "          3.5527e-13,  1.5348e-12,  3.0127e-12,  7.1054e-13,  3.6380e-12,\n",
       "          1.8190e-12, -3.9790e-12, -3.1086e-15,  1.3074e-12,  1.9895e-13,\n",
       "         -1.4566e-13, -2.9559e-12,  1.0516e-12,  1.1937e-12,  1.4211e-14,\n",
       "         -1.3074e-12],\n",
       "        [-2.2204e-13, -2.0428e-13, -1.4211e-12,  2.7617e-15, -1.6342e-13,\n",
       "          1.8076e-11,  9.4929e-12, -7.9581e-12, -8.5265e-13,  5.2296e-12,\n",
       "          1.0459e-11,  2.4443e-12,  0.0000e+00, -1.2449e-11, -5.2296e-12,\n",
       "         -1.7764e-12,  2.2737e-13, -1.4097e-11, -5.0022e-12, -1.9440e-11,\n",
       "         -3.9790e-12,  2.4443e-11,  1.9984e-14,  1.3642e-12, -1.4566e-12,\n",
       "          2.0606e-13,  1.2164e-11, -1.9327e-12, -2.2737e-12,  7.4607e-14,\n",
       "          5.7412e-12],\n",
       "        [-1.7347e-16, -1.5266e-16, -1.1102e-15,  2.1142e-18, -1.3531e-16,\n",
       "          1.4544e-14,  7.6605e-15, -6.2172e-15, -4.4409e-16,  4.3299e-15,\n",
       "          8.6597e-15,  2.0262e-15,  0.0000e+00, -9.6589e-15, -4.3299e-15,\n",
       "         -1.4294e-15,  6.6613e-16, -1.1269e-14, -4.1078e-15, -1.5765e-14,\n",
       "         -3.1086e-15,  1.9984e-14,  1.6697e-17,  1.5543e-15, -1.1796e-15,\n",
       "          1.2490e-16,  9.4369e-15, -1.3878e-15, -1.7764e-15,  6.5919e-17,\n",
       "          4.5519e-15],\n",
       "        [ 6.3949e-14,  4.2633e-14,  3.6948e-13, -5.5511e-16, -1.4211e-14,\n",
       "          1.0232e-12, -9.0949e-13,  2.7285e-12,  1.3642e-12,  4.5475e-13,\n",
       "          9.0949e-13,  1.4211e-13,  0.0000e+00,  1.5348e-12, -4.5475e-13,\n",
       "          2.1316e-14,  2.8990e-12,  5.6843e-13, -4.8317e-13, -2.8422e-13,\n",
       "          1.3074e-12,  1.3642e-12,  1.5543e-15,  2.9559e-12, -9.2371e-14,\n",
       "         -2.0961e-13, -1.3642e-12,  1.2506e-12,  9.0949e-13,  4.2633e-14,\n",
       "         -3.9790e-13],\n",
       "        [ 1.1990e-14,  1.2990e-14,  7.9936e-14, -1.7174e-16,  9.6589e-15,\n",
       "         -1.1582e-12, -5.1514e-13,  3.8369e-13,  6.0396e-14, -3.0909e-13,\n",
       "         -6.1817e-13, -1.4566e-13,  0.0000e+00,  7.6739e-13,  3.0909e-13,\n",
       "          1.0658e-13, -1.7764e-14,  8.4732e-13,  3.0020e-13,  1.1617e-12,\n",
       "          1.9895e-13, -1.4566e-12, -1.1796e-15, -9.2371e-14,  9.1704e-14,\n",
       "         -1.3545e-14, -7.5673e-13,  1.0836e-13,  9.5923e-14, -5.1070e-15,\n",
       "         -3.4639e-13],\n",
       "        [-7.4385e-15, -6.6613e-15, -4.6185e-14,  8.5869e-17, -8.8818e-16,\n",
       "          1.8829e-13,  1.8829e-13, -3.0553e-13, -1.2790e-13,  2.8422e-14,\n",
       "          5.6843e-14,  2.0428e-14,  0.0000e+00, -3.1264e-13, -2.8422e-14,\n",
       "         -2.5535e-14, -2.4158e-13, -2.3981e-13, -2.8422e-14, -2.3270e-13,\n",
       "         -1.4566e-13,  2.0606e-13,  1.2490e-16, -2.0961e-13, -1.3545e-14,\n",
       "          2.1760e-14,  3.0198e-13, -1.1902e-13, -8.5265e-14, -2.3315e-15,\n",
       "          1.1280e-13],\n",
       "        [-1.5987e-13, -1.5277e-13, -1.0232e-12,  2.0123e-15, -7.6383e-14,\n",
       "          9.6634e-12,  5.6843e-12, -6.0254e-12, -1.6200e-12,  2.4443e-12,\n",
       "          4.8885e-12,  1.2079e-12,  0.0000e+00, -8.4128e-12, -2.4443e-12,\n",
       "         -9.9476e-13, -2.2169e-12, -8.2423e-12, -2.3874e-12, -1.0402e-11,\n",
       "         -2.9559e-12,  1.2164e-11,  9.4369e-15, -1.3642e-12, -7.5673e-13,\n",
       "          3.0198e-13,  8.2423e-12, -1.9043e-12, -1.6485e-12,  1.0658e-14,\n",
       "          3.5101e-12],\n",
       "        [ 5.4179e-14,  4.0856e-14,  3.2685e-13, -5.4817e-16,  1.0658e-14,\n",
       "         -1.3927e-12, -1.4495e-12,  2.1600e-12,  7.2475e-13, -3.4106e-13,\n",
       "         -6.8212e-13, -1.9185e-13,  0.0000e+00,  2.0179e-12,  3.4106e-13,\n",
       "          1.9718e-13,  1.3358e-12,  1.7479e-12,  2.9843e-13,  1.9185e-12,\n",
       "          1.0516e-12, -1.9327e-12, -1.3878e-15,  1.2506e-12,  1.0836e-13,\n",
       "         -1.1902e-13, -1.9043e-12,  7.9581e-13,  6.8212e-13,  1.2434e-14,\n",
       "         -7.8870e-13],\n",
       "        [ 5.6843e-14,  3.3751e-14,  3.2685e-13, -4.4409e-16,  1.4211e-14,\n",
       "         -1.0800e-12, -1.8190e-12,  2.6148e-12,  4.5475e-13, -4.5475e-13,\n",
       "         -9.0949e-13, -2.2737e-13,  0.0000e+00,  1.8190e-12,  4.5475e-13,\n",
       "          2.0961e-13,  9.0949e-13,  1.7906e-12,  3.6948e-13,  2.1316e-12,\n",
       "          1.1937e-12, -2.2737e-12, -1.7764e-15,  9.0949e-13,  9.5923e-14,\n",
       "         -8.5265e-14, -1.6485e-12,  6.8212e-13,  1.0232e-12,  1.2434e-14,\n",
       "         -7.8160e-13],\n",
       "        [ 4.9960e-16,  0.0000e+00,  2.6645e-15, -4.3368e-19, -5.5511e-16,\n",
       "          6.3949e-14,  1.7764e-15,  3.5527e-14,  1.5099e-14,  1.7764e-14,\n",
       "          3.5527e-14,  7.5495e-15,  0.0000e+00, -8.8818e-15, -1.7764e-14,\n",
       "         -3.7748e-15,  4.0856e-14, -2.3981e-14, -1.7764e-14, -4.7962e-14,\n",
       "          1.4211e-14,  7.4607e-14,  6.5919e-17,  4.2633e-14, -5.1070e-15,\n",
       "         -2.3315e-15,  1.0658e-14,  1.2434e-14,  1.2434e-14,  8.8818e-16,\n",
       "          7.5495e-15],\n",
       "        [-7.0610e-14, -6.4393e-14, -4.4764e-13,  8.5001e-16, -3.6859e-14,\n",
       "          4.3627e-12,  2.5864e-12, -2.6716e-12, -5.8265e-13,  1.1795e-12,\n",
       "          2.3590e-12,  5.7199e-13,  0.0000e+00, -3.6096e-12, -1.1795e-12,\n",
       "         -4.5297e-13, -7.1054e-13, -3.7232e-12, -1.1298e-12, -4.8033e-12,\n",
       "         -1.3074e-12,  5.7412e-12,  4.5519e-15, -3.9790e-13, -3.4639e-13,\n",
       "          1.1280e-13,  3.5101e-12, -7.8870e-13, -7.8160e-13,  7.5495e-15,\n",
       "          1.5667e-12]])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(D_hat, D_hat.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 20])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Probability\n",
    "- There is a small problem here in the variance of Normal but I will fix it today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_probs = np.zeros((X_test.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([1.])\n",
      "tensor([0.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "prob = 0\n",
    "for i, (x, y) in enumerate(zip(X_test, y_test)):\n",
    "    x = torch.cat((x, torch.tensor([1], dtype=torch.float32)), 0)\n",
    "#     print(f'y = {y}')\n",
    "#     print(f'mean = {torch.matmul(theta_swa, x)}')\n",
    "#     print(f'var = { 1/2 * torch.matmul(torch.matmul(x, (sigma_diag + torch.matmul(D_hat, D_hat.transpose(0, 1)) / (K - 1))), x)}')\n",
    "    res = y + (1 - 2 * y) * \\\n",
    "        Normal(torch.matmul(theta_swa, x),  1/2 * torch.matmul(torch.matmul(x, (torch.eye(sigma_diag.shape[0]) * sigma_diag + torch.matmul(D_hat, D_hat.transpose(0, 1)) / (K - 1))), x)).cdf(torch.tensor(0, dtype=torch.float32))\n",
    "    prob += res\n",
    "    print(res)\n",
    "    y = int(y)\n",
    "    all_probs[i, y] = res\n",
    "    all_probs[i, 1 - y] = 1 - res\n",
    "    \n",
    "prob /= X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9737])"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivante sert à afficher la courbe de calibration : confidence - accuracy en fonction de la confidence. Par exemple on peut comparer les courbes obtenus avec les différentes méthodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_curve(all_probs, ytest, num_bins = 1):\n",
    "    prob_preds = np.max(all_probs, 1)\n",
    "    predictions = np.argmax(all_probs, 1)\n",
    "    ind_order = np.argsort(prob_preds)\n",
    "    ind_bins = np.array_split(ind_order, num_bins)\n",
    "    bins = np.array_split(prob_preds[ind_order], num_bins)\n",
    "    accuracies = (predictions == ytest)\n",
    "    accuracy_bin_array = np.zeros(num_bins)\n",
    "    confidence_bin_array = np.zeros(num_bins)\n",
    "    for it, bin_prob in enumerate(bins):\n",
    "        accuracy_bin = accuracies[ind_bins[it]].mean()\n",
    "        confidence_bin = bin_prob.mean()\n",
    "        accuracy_bin_array[it] = accuracy_bin\n",
    "        confidence_bin_array[it] = confidence_bin\n",
    "    return accuracy_bin_array, confidence_bin_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1481eed5070>]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# all_probs = # on veut une matrice : (Nb_data, dim_output) contenant les probabilités de cahque classe\n",
    "ytest = np.squeeze(np.array(y_test))\n",
    "# Compute the calibration curve\n",
    "cal_curve = calibration_curve(all_probs, ytest, num_bins=20)\n",
    "# Display the result\n",
    "plt.plot(cal_curve[1], cal_curve[1] - cal_curve[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That means it is less confident than what it actually is?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
